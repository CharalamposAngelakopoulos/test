{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):\n",
    "    n_channels_int = n_channels[0]\n",
    "    inp = input_a + input_b\n",
    "    t_act = torch.tanh(inp[:,:n_channels_int,:])\n",
    "    s_act = torch.sigmoid(inp[:,n_channels_int:,:])\n",
    "    acts = t_act * s_act\n",
    "    return acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 100])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Invertible1x1Conv(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super(Invertible1x1Conv, self).__init__()\n",
    "        self.conv = nn.Conv1d(c,c,1, bias=False)\n",
    "        w = self.conv.weight.data\n",
    "        nn.init.orthogonal_(w)\n",
    "        if w.squeeze().det() < 0:\n",
    "            w[0,:,:] = -w[0,:,:]\n",
    "        \n",
    "        self.conv.weight.data = w\n",
    "\n",
    "    def forward(self, z, reverse=False):\n",
    "        # z: [BS, n_group, L]\n",
    "        W = self.conv.weight.data\n",
    "        if reverse:\n",
    "            W_inverse = W.squeeze().inverse().unsqueeze(-1)\n",
    "            z = F.conv1d(z, W_inverse)\n",
    "            return z\n",
    "        else:\n",
    "            log_det_W = W.squeeze().logdet()\n",
    "            return self.conv(z), log_det_W\n",
    "\n",
    "\n",
    "inv = Invertible1x1Conv(8)\n",
    "inv(torch.randn(1,8,100), True).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 100])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WN(nn.Module):\n",
    "    def __init__(self, n_in_channels, n_mel_channels, n_channels, n_layers, \n",
    "                kernel_size):\n",
    "        super(WN, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_layers = n_layers\n",
    "        self.start = nn.Conv1d(n_in_channels, n_channels, 1)\n",
    "        self.cond_layer = nn.Conv1d(n_mel_channels, 2*n_channels*n_layers,1)\n",
    "        self.in_layers = nn.ModuleList()\n",
    "        self.res_skip_layers = nn.ModuleList()\n",
    "        self.end = nn.Conv1d(n_channels, 2*n_in_channels, 1)\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            dil = 2**i\n",
    "            padding = int(dil*(kernel_size-1)/2)\n",
    "            in_layer = nn.Conv1d(n_channels, 2*n_channels, kernel_size,\n",
    "                                dilation=dil, padding=padding)\n",
    "            \n",
    "            self.in_layers.append(in_layer)\n",
    "\n",
    "            if i == n_layers - 1:\n",
    "                res_skip_layer = nn.Conv1d(n_channels, n_channels, 1)\n",
    "            else:\n",
    "                res_skip_layer = nn.Conv1d(n_channels, 2*n_channels, 1)\n",
    "            \n",
    "            self.res_skip_layers.append(res_skip_layer)\n",
    "    \n",
    "\n",
    "    def forward(self, forward_input):\n",
    "        audio, spect = forward_input\n",
    "        # audio: [BS, half, L]  \n",
    "        # spect: [BS, n_mel_channels, L]  \n",
    "        audio = self.start(audio)\n",
    "        spect = self.cond_layer(spect)\n",
    "        output = torch.zeros_like(audio)\n",
    "        for i in range(self.n_layers):\n",
    "            inp = self.in_layers[i](audio)\n",
    "            spect_offset = 2*i*self.n_channels\n",
    "            new_spect = spect[:,spect_offset:spect_offset + 2*self.n_channels,:]\n",
    "            n_channels_int = torch.IntTensor([self.n_channels])\n",
    "\n",
    "            acts = fused_add_tanh_sigmoid_multiply(\n",
    "                inp,\n",
    "                new_spect,\n",
    "                n_channels_int)\n",
    "\n",
    "            res_acts = self.res_skip_layers[i](acts)\n",
    "            \n",
    "            \n",
    "\n",
    "            if i == self.n_layers - 1:\n",
    "                output = output + res_acts\n",
    "            else:\n",
    "                audio = audio + res_acts[:,:self.n_channels,:]\n",
    "                output = output + res_acts[:,self.n_channels:,:]\n",
    "\n",
    "        output = self.end(output)\n",
    "        return output\n",
    "\n",
    "    \n",
    "\n",
    "wn = WN(4,80,256,8,3)\n",
    "wn.infer(torch.randn(2, 80, 100)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 1184]) torch.Size([1, 2, 1184])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [256, 2, 1], expected input[1, 3, 1184] to have 2 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-202-5e8d1e64a955>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[0mspect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m37\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-202-5e8d1e64a955>\u001b[0m in \u001b[0;36minfer\u001b[1;34m(self, spect, sigma)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m                 \u001b[0mwn_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWN\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m                 \u001b[0mwn_channels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwn_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[0mlog_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwn_out\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwn_channels\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-126-6f215e54263a>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, forward_input)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# audio: [BS, half, L]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# spect: [BS, n_mel_channels, L]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0maudio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mspect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcond_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    295\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m--> 297\u001b[1;33m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    298\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [256, 2, 1], expected input[1, 3, 1184] to have 2 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "class WaveGlow(nn.Module):\n",
    "    def __init__(self, n_mel_channels, n_group, n_flows, n_channels,\n",
    "                n_layers, kernel_size, n_early_every, n_early_size):\n",
    "        super(WaveGlow, self).__init__()\n",
    "        \n",
    "        self.n_group = n_group\n",
    "        self.n_flows = n_flows\n",
    "        self.n_early_every = n_early_every\n",
    "        self.n_early_size = n_early_size\n",
    "\n",
    "        # upsample spect to audio scale\n",
    "        self.upsample = nn.ConvTranspose1d(n_mel_channels, n_mel_channels,\n",
    "                                           1024, stride=256)\n",
    "\n",
    "        self.convinv = nn.ModuleList()\n",
    "        self.WN = nn.ModuleList()\n",
    "\n",
    "        remaining_channels = n_group\n",
    "        half = int(remaining_channels/2)\n",
    "        for k in range(n_flows):\n",
    "                if k % n_early_every == 0 and k > 0:\n",
    "                        remaining_channels = remaining_channels - n_early_size\n",
    "                        half = int(remaining_channels/2)\n",
    "\n",
    "                inv = Invertible1x1Conv(remaining_channels)\n",
    "                wn = WN(half, n_mel_channels*n_group, n_channels, n_layers, \n",
    "                        kernel_size)\n",
    "\n",
    "                self.convinv.append(inv)\n",
    "                self.WN.append(wn)\n",
    "        self.n_remaining_channels = remaining_channels  # Useful during inference\n",
    "    def forward(self, forward_input):\n",
    "        audio, spect = forward_input\n",
    "        # audio: [BS, time]\n",
    "        # spect: [BS, n_mel_channels, frames]\n",
    "\n",
    "        # upsample spect to audio scale --->[BS, n_mel_channels, time]\n",
    "        spect = self.upsample(spect)\n",
    "        \n",
    "        assert(spect.size(2) >= audio.size(1))\n",
    "        if spect.size(2) > audio.size(1):\n",
    "            spect = spect[:, :, :audio.size(1)]\n",
    "\n",
    "        spect = spect.unfold(2, self.n_group, self.n_group).permute(0, 2, 1, 3)\n",
    "        \n",
    "        spect = spect.contiguous().view(spect.size(0), spect.size(1), -1).permute(0, 2, 1)\n",
    "        \n",
    "        audio = audio.unfold(1, self.n_group, self.n_group).permute(0, 2, 1)\n",
    "        \n",
    "        output_audio = []\n",
    "        log_det_W_list = []\n",
    "        log_s_list = []\n",
    "\n",
    "        for k in range(self.n_flows):\n",
    "                if k % self.n_early_every == 0 and k > 0:\n",
    "                        output_audio.append(audio[:,:self.n_early_size,:])\n",
    "                        audio = audio[:,self.n_early_size:,:]\n",
    "                \n",
    "                audio, log_det_W = self.convinv[k](audio)\n",
    "                log_det_W_list.append(log_det_W)\n",
    "\n",
    "                # split audio for affine coupling layer\n",
    "                audio_channels = audio.shape[1]\n",
    "                audio_0 = audio[:,:int(audio_channels/2),:]\n",
    "                audio_1 = audio[:,int(audio_channels/2):,:]\n",
    "\n",
    "                wn_out = self.WN[k]((audio_0, spect))\n",
    "                wn_out_channels = wn_out.shape[1]\n",
    "                log_s = wn_out[:,:int(wn_out_channels/2),:]\n",
    "                \n",
    "                log_s_list.append(log_s)\n",
    "                t = wn_out[:,int(wn_out_channels/2):,:]\n",
    "\n",
    "                audio_1 = torch.exp(log_s)*audio_1 + t\n",
    "\n",
    "                audio = torch.cat([audio_0, audio_1], dim=1)\n",
    "\n",
    "        output_audio.append(audio)\n",
    "        return torch.cat(output_audio, 1), log_s_list, log_det_W_list\n",
    "\n",
    "    def infer(self, spect, sigma=1.0):\n",
    "        spect = self.upsample(spect)\n",
    "        time_cutoff = self.upsample.kernel_size[0] - self.upsample.stride[0]\n",
    "        spect = spect[:,:,:-time_cutoff]\n",
    "        spect = spect.unfold(2, self.n_group, self.n_group).permute(0, 2, 1, 3)\n",
    "        spect = spect.contiguous().view(spect.size(0), spect.size(1), -1).permute(0, 2, 1)\n",
    "\n",
    "        audio = torch.randn(spect.shape[0], self.n_remaining_channels,\n",
    "                            spect.shape[2]).normal_()\n",
    "        \n",
    "        audio = sigma*audio\n",
    "\n",
    "        for k in reversed(range(self.n_flows)):\n",
    "                half = int(audio.shape[1]/2)\n",
    "                audio_0 = audio[:,:half,:]\n",
    "                audio_1 = audio[:,half:,:]\n",
    "\n",
    "                \n",
    "                wn_out = self.WN[k]((audio_0, spect))\n",
    "                wn_channels = wn_out.shape[1]\n",
    "                log_s = wn_out[:,:int(wn_channels/2),:]\n",
    "                t = wn_out[:,int(wn_channels/2):,:]\n",
    "                x0 = audio_0\n",
    "                x1 = (audio_1 - t)/torch.exp(log_s)\n",
    "                print(x0.shape, x1.shape)\n",
    "                inv_inp = torch.cat([x0, x1], dim=1)\n",
    "                \n",
    "                audio = self.convinv[k](inv_inp, reverse=True)\n",
    "\n",
    "                if k % self.n_early_every and k > 0:\n",
    "                        z = torch.randn(spect.shape[0], self.n_early_size, spect.shape[2]).normal_()\n",
    "                        audio = torch.cat([sigma*z, audio], 1)\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "        return audio\n",
    "\n",
    "model = WaveGlow(80, 8, 12, 256, 8, 3, 4, 2)\n",
    "\n",
    "audio = torch.randn(1, 10000)\n",
    "spect = torch.randn(1, 80, 37)\n",
    "\n",
    "model.infer(spect).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 640, 1250])\n",
      "torch.Size([1, 640, 1250])\n",
      "torch.Size([1, 640, 1250])\n",
      "torch.Size([1, 640, 1250])\n",
      "torch.Size([1, 640, 1250])\n",
      "torch.Size([1, 640, 1250])\n",
      "torch.Size([1, 640, 1250])\n",
      "torch.Size([1, 640, 1250])\n",
      "torch.Size([1, 640, 1250])\n",
      "torch.Size([1, 640, 1250])\n",
      "torch.Size([1, 640, 1250])\n",
      "torch.Size([1, 640, 1250])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.2107, -2.5223, -0.8095,  ..., -1.4856, -0.0048,  0.8866],\n",
       "          [-0.0582, -1.2682,  0.7698,  ..., -0.9491, -1.0466,  0.5091],\n",
       "          [-0.3005,  0.5528, -0.4263,  ..., -2.2444, -1.5032, -0.8932],\n",
       "          ...,\n",
       "          [-1.0784, -0.7570,  0.9992,  ...,  0.1646,  0.9503,  0.2673],\n",
       "          [-1.2106, -0.0477, -0.0941,  ..., -0.9875, -2.1894, -0.0410],\n",
       "          [ 0.8242,  0.7685,  0.0804,  ..., -0.5033, -0.4835,  0.7518]]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " [tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<SliceBackward0>),\n",
       "  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<SliceBackward0>),\n",
       "  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<SliceBackward0>),\n",
       "  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<SliceBackward0>),\n",
       "  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<SliceBackward0>),\n",
       "  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<SliceBackward0>),\n",
       "  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<SliceBackward0>),\n",
       "  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<SliceBackward0>),\n",
       "  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<SliceBackward0>),\n",
       "  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<SliceBackward0>),\n",
       "  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<SliceBackward0>),\n",
       "  tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<SliceBackward0>)],\n",
       " [tensor(2.9802e-08),\n",
       "  tensor(-3.2783e-07),\n",
       "  tensor(-5.9605e-08),\n",
       "  tensor(-8.9407e-08),\n",
       "  tensor(1.1921e-07),\n",
       "  tensor(-1.1921e-07),\n",
       "  tensor(1.4901e-08),\n",
       "  tensor(0.),\n",
       "  tensor(5.9605e-08),\n",
       "  tensor(-5.2154e-08),\n",
       "  tensor(-1.1921e-07),\n",
       "  tensor(-1.1921e-07)])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WN2(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This is the WaveNet like layer for the affine coupling.  The primary difference\n",
    "    from WaveNet is the convolutions need not be causal.  There is also no dilation\n",
    "    size reset.  The dilation only doubles on each layer\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in_channels, n_mel_channels, n_layers, n_channels,\n",
    "                 kernel_size):\n",
    "        super(WN2, self).__init__()\n",
    "        assert(kernel_size % 2 == 1)\n",
    "        assert(n_channels % 2 == 0)\n",
    "        self.n_layers = n_layers\n",
    "        self.n_channels = n_channels\n",
    "        self.in_layers = torch.nn.ModuleList()\n",
    "        self.res_skip_layers = torch.nn.ModuleList()\n",
    "\n",
    "        start = torch.nn.Conv1d(n_in_channels, n_channels, 1)\n",
    "        start = torch.nn.utils.weight_norm(start, name='weight')\n",
    "        self.start = start\n",
    "\n",
    "        # Initializing last layer to 0 makes the affine coupling layers\n",
    "        # do nothing at first.  This helps with training stability\n",
    "        end = torch.nn.Conv1d(n_channels, 2*n_in_channels, 1)\n",
    "        end.weight.data.zero_()\n",
    "        end.bias.data.zero_()\n",
    "        self.end = end\n",
    "\n",
    "        cond_layer = torch.nn.Conv1d(n_mel_channels, 2*n_channels*n_layers, 1)\n",
    "        self.cond_layer = torch.nn.utils.weight_norm(cond_layer, name='weight')\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            dilation = 2 ** i\n",
    "            padding = int((kernel_size*dilation - dilation)/2)\n",
    "            in_layer = torch.nn.Conv1d(n_channels, 2*n_channels, kernel_size,\n",
    "                                       dilation=dilation, padding=padding)\n",
    "            in_layer = torch.nn.utils.weight_norm(in_layer, name='weight')\n",
    "            self.in_layers.append(in_layer)\n",
    "\n",
    "\n",
    "            # last one is not necessary\n",
    "            if i < n_layers - 1:\n",
    "                res_skip_channels = 2*n_channels\n",
    "            else:\n",
    "                res_skip_channels = n_channels\n",
    "            res_skip_layer = torch.nn.Conv1d(n_channels, res_skip_channels, 1)\n",
    "            res_skip_layer = torch.nn.utils.weight_norm(res_skip_layer, name='weight')\n",
    "            self.res_skip_layers.append(res_skip_layer)\n",
    "\n",
    "    def forward(self, forward_input):\n",
    "        audio, spect = forward_input\n",
    "        audio = self.start(audio)\n",
    "        output = torch.zeros_like(audio)\n",
    "        n_channels_tensor = torch.IntTensor([self.n_channels])\n",
    "\n",
    "        \n",
    "        spect = self.cond_layer(spect)\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            spect_offset = i*2*self.n_channels\n",
    "            acts = fused_add_tanh_sigmoid_multiply(\n",
    "                self.in_layers[i](audio),\n",
    "                spect[:,spect_offset:spect_offset+2*self.n_channels,:],\n",
    "                n_channels_tensor)\n",
    "\n",
    "            res_skip_acts = self.res_skip_layers[i](acts)\n",
    "            if i < self.n_layers - 1:\n",
    "                audio = audio + res_skip_acts[:,:self.n_channels,:]\n",
    "                output = output + res_skip_acts[:,self.n_channels:,:]\n",
    "            else:\n",
    "                output = output + res_skip_acts\n",
    "\n",
    "        return self.end(output)\n",
    "\n",
    "\n",
    "class WaveGlow2(torch.nn.Module):\n",
    "    def __init__(self, n_mel_channels, n_flows, n_group, n_early_every,\n",
    "                 n_early_size, WN_config):\n",
    "        super(WaveGlow2, self).__init__()\n",
    "\n",
    "        self.upsample = torch.nn.ConvTranspose1d(n_mel_channels,\n",
    "                                                 n_mel_channels,\n",
    "                                                 1024, stride=256)\n",
    "        assert(n_group % 2 == 0)\n",
    "        self.n_flows = n_flows\n",
    "        self.n_group = n_group\n",
    "        self.n_early_every = n_early_every\n",
    "        self.n_early_size = n_early_size\n",
    "        self.WN2 = torch.nn.ModuleList()\n",
    "        self.convinv = torch.nn.ModuleList()\n",
    "\n",
    "        n_half = int(n_group/2)\n",
    "\n",
    "        # Set up layers with the right sizes based on how many dimensions\n",
    "        # have been output already\n",
    "        n_remaining_channels = n_group\n",
    "        for k in range(n_flows):\n",
    "            if k % self.n_early_every == 0 and k > 0:\n",
    "                n_half = n_half - int(self.n_early_size/2)\n",
    "                n_remaining_channels = n_remaining_channels - self.n_early_size\n",
    "            self.convinv.append(Invertible1x1Conv(n_remaining_channels))\n",
    "            self.WN2.append(WN2(n_half, n_mel_channels*n_group, **WN_config))\n",
    "        self.n_remaining_channels = n_remaining_channels  # Useful during inference\n",
    "\n",
    "    def forward(self, forward_input):\n",
    "        \"\"\"\n",
    "        forward_input[0] = mel_spectrogram:  batch x n_mel_channels x frames\n",
    "        forward_input[1] = audio: batch x time\n",
    "        \"\"\"\n",
    "        spect, audio = forward_input\n",
    "\n",
    "        #  Upsample spectrogram to size of audio\n",
    "        spect = self.upsample(spect)\n",
    "        \n",
    "        \n",
    "        assert(spect.size(2) >= audio.size(1))\n",
    "        if spect.size(2) > audio.size(1):\n",
    "            spect = spect[:, :, :audio.size(1)]\n",
    "\n",
    "        \n",
    "\n",
    "        spect = spect.unfold(2, self.n_group, self.n_group).permute(0, 2, 1, 3)\n",
    "        spect = spect.contiguous().view(spect.size(0), spect.size(1), -1).permute(0, 2, 1)\n",
    "\n",
    "        audio = audio.unfold(1, self.n_group, self.n_group).permute(0, 2, 1)\n",
    "        \n",
    "        output_audio = []\n",
    "        log_s_list = []\n",
    "        log_det_W_list = []\n",
    "\n",
    "        for k in range(self.n_flows):\n",
    "            if k % self.n_early_every == 0 and k > 0:\n",
    "                output_audio.append(audio[:,:self.n_early_size,:])\n",
    "                audio = audio[:,self.n_early_size:,:]\n",
    "\n",
    "            audio, log_det_W = self.convinv[k](audio)\n",
    "            log_det_W_list.append(log_det_W)\n",
    "\n",
    "            n_half = int(audio.size(1)/2)\n",
    "            audio_0 = audio[:,:n_half,:]\n",
    "            audio_1 = audio[:,n_half:,:]\n",
    "\n",
    "            \n",
    "            \n",
    "            output = self.WN2[k]((audio_0, spect))\n",
    "            log_s = output[:, n_half:, :]\n",
    "            b = output[:, :n_half, :]\n",
    "            audio_1 = torch.exp(log_s)*audio_1 + b\n",
    "            log_s_list.append(log_s)\n",
    "\n",
    "            audio = torch.cat([audio_0, audio_1],1)\n",
    "\n",
    "        output_audio.append(audio)\n",
    "        return torch.cat(output_audio,1), log_s_list, log_det_W_list\n",
    "\n",
    "\n",
    "WN_config= {\n",
    "            \"n_layers\": 8,\n",
    "            \"n_channels\": 256,\n",
    "            \"kernel_size\": 3\n",
    "        }\n",
    "wv2 = WaveGlow2(80, 12, 8, 4, 2, WN_config)\n",
    "\n",
    "audio = torch.randn(1, 10000)\n",
    "spect = torch.randn(1, 80, 37)\n",
    "wv2((spect,audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.0625"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(10000-1024)/256+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mel2Samp(Dataset):\n",
    "    \"\"\"\n",
    "    This is the main class that calculates the spectrogram and returns the\n",
    "    spectrogram, audio pair.\n",
    "    \"\"\"\n",
    "    def __init__(self, training_files, segment_length, filter_length=1024, \n",
    "                 hop_length=256, win_length=1024, n_mel_channels=80,\n",
    "                 sampling_rate=22050, mel_fmin=0.0, mel_fmax=8000.0):\n",
    "\n",
    "        super(Mel2Samp, self).__init__()\n",
    "        self.audio_files = files_to_list(training_files)\n",
    "        self.stft = TacotronSTFT(filter_length, hop_length, win_length,\n",
    "                n_mel_channels, sampling_rate, mel_fmin, mel_fmax)\n",
    "        self.segment_length = segment_length\n",
    "\n",
    "    def get_mel(self, audio):\n",
    "        audio_norm = audio/MAX_WAV_VALUE\n",
    "        audio_norm = audio_norm.unsqueeze(0)\n",
    "        melspec = self.stft.mel_spectrogram(audio_norm)\n",
    "        melspec = melspec.squeeze(0)\n",
    "        return melspec\n",
    "\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        audio_file = self.audio_files[index]\n",
    "        audio, sr = load_wav_to_torch(audio_file)\n",
    "        \n",
    "        if audio.size(0) >= self.segment_length:\n",
    "            max_audio_start = audio.size(0) - self.segment_length\n",
    "            audio_start = torch.randint(0, max_audio_start, [1])\n",
    "            audio = audio[audio_start:audio_start + self.segment_length]\n",
    "        else:\n",
    "            audio = F.pad(audio, (0, self.segment_length - audio.size(0)))\n",
    "\n",
    "        mel = self.get_mel(audio)\n",
    "        audio = audio/MAX_WAV_VALUE\n",
    "        return (mel, audio)\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Mel2Samp(r\"C:\\Users\\Codefactory\\Documents\\babis\\Thesis\\Waveglow\\test.txt\", 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 80, 63]), torch.Size([1, 16000]))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "mel, audio = next(iter(loader))\n",
    "\n",
    "mel.shape, audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-121-784eba11f61b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-119-02f9adce5ffe>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, forward_input)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m# spect: [BS, n_mel_channels, frames]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0maudio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[1;31m# squeeze audio to vectors ---> [BS, n_group, L]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0maudio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maudio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munfold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_group\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_group\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model((audio, mel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9414e94257f5df456230ea9212a9d2caf7ef208e33124ce6f17f0ac694c2c858"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
