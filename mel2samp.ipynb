{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "from scipy.io.wavfile import read\n",
    "sys.path.insert(0, r\"C:\\Users\\Codefactory\\Documents\\babis\\Thesis\\tacotron2\")\n",
    "from layers import TacotronSTFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_WAV_VALUE = 32768.0\n",
    "\n",
    "def files_to_list(filename):\n",
    "    \"\"\"\n",
    "    Takes a text file of filenames and makes a list of filenames\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "        filenames = f.read().strip().splitlines()\n",
    "            \n",
    "\n",
    "    return filenames\n",
    "\n",
    "\n",
    "def load_wav_to_torch(full_path):\n",
    "    \"\"\"\n",
    "    Loads wavdata into torch array\n",
    "    \"\"\"\n",
    "\n",
    "    sr, data = read(full_path)\n",
    "    wav_data = torch.tensor(data).float()\n",
    "    return  wav_data, sr\n",
    "\n",
    "\n",
    "a = load_wav_to_torch(r\"C:\\Users\\Codefactory\\Documents\\babis\\Tutorials\\Deep Learning\\Pytorch\\waves_yesno\\0_0_0_0_1_1_1_1.wav\")[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Mel2Samp(Dataset):\n",
    "    \"\"\"\n",
    "    This is the main class that calculates the spectrogram and returns the\n",
    "    spectrogram, audio pair.\n",
    "    \"\"\"\n",
    "    def __init__(self, training_files, segment_length, filter_length=1024, \n",
    "                 hop_length=256, win_length=1024, n_mel_channels=80,\n",
    "                 sampling_rate=22050, mel_fmin=0.0, mel_fmax=8000.0):\n",
    "\n",
    "        super(Mel2Samp, self).__init__()\n",
    "        self.audio_files = files_to_list(training_files)\n",
    "        self.stft = TacotronSTFT(filter_length, hop_length, win_length,\n",
    "                n_mel_channels, sampling_rate, mel_fmin, mel_fmax)\n",
    "        self.segment_length = segment_length\n",
    "\n",
    "    def get_mel(self, audio):\n",
    "        audio_norm = audio/MAX_WAV_VALUE\n",
    "        audio_norm = audio_norm.unsqueeze(0)\n",
    "        melspec = self.stft.mel_spectrogram(audio_norm)\n",
    "        melspec = melspec.squeeze(0)\n",
    "        return melspec\n",
    "\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        audio_file = self.audio_files[index]\n",
    "        audio, sr = load_wav_to_torch(audio_file)\n",
    "        \n",
    "        if audio.size(0) >= self.segment_length:\n",
    "            max_audio_start = audio.size(0) - self.segment_length\n",
    "            audio_start = torch.randint(0, max_audio_start, 1)\n",
    "            audio = audio[audio_start:audio_start + self.segment_length]\n",
    "        else:\n",
    "            audio = F.pad(audio, (0, self.segment_length - audio.size(0)))\n",
    "\n",
    "        mel = self.get_mel(audio)\n",
    "        audio = audio/MAX_WAV_VALUE\n",
    "        return (mel, audio)\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "d = Mel2Samp(r\"C:\\Users\\Codefactory\\Documents\\babis\\Thesis\\Waveglow\\test.txt\", 60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 80, 235]) torch.Size([1, 60000])\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(d, 1, shuffle=True)\n",
    "\n",
    "x,y = next(iter(loader))\n",
    "\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fused_add_tanh_sigmoid(input_a, input_b, n_channels):\n",
    "    n_channels_int = n_channels[0]\n",
    "    in_act = input_a + input_b\n",
    "    t_act = torch.tanh(in_act[:,:n_channels_int,:])\n",
    "    s_act = torch.sigmoid(in_act[:,n_channels_int:,:])\n",
    "    print(t_act.shape, s_act.shape, in_act.shape)\n",
    "    acts = t_act*s_act\n",
    "    return acts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Invertible1x1Conv(nn.Module):\n",
    "    \"\"\"\n",
    "    The layer outputs both the convolution, and the log determinant\n",
    "    of its weight matrix.  If reverse=True it does convolution with\n",
    "    inverse\n",
    "    \"\"\"\n",
    "    def __init__(self, c):\n",
    "        super(Invertible1x1Conv, self).__init__()\n",
    "        self.conv = nn.Conv1d(c, c, kernel_size=1, bias=False)\n",
    "        W = torch.qr(torch.FloatTensor(c,c).normal_())[0]\n",
    "\n",
    "        # ensure det=1\n",
    "        if W.det()<0:\n",
    "            W = -W\n",
    "        W = W.view(c,c,1)\n",
    "        self.conv.weight.data = W\n",
    "\n",
    "\n",
    "    def forward(self, z, reverse=False):\n",
    "        BS, group_size, n_of_groups = z.shape\n",
    "        W = self.conv.weight.squeeze()\n",
    "        if reverse:\n",
    "            if not hasattr(self, 'W_inverse'):\n",
    "                W_inverse = W.float().inverse()\n",
    "                W_inverse = W_inverse.unsqueeze(-1)\n",
    "                if z.type() == 'torch.cuda.HalfTensor':\n",
    "                    W_inverse = W_inverse.half()\n",
    "                self.W_inverse = W_inverse \n",
    "            z = F.conv1d(z, W_inverse, bias=None)\n",
    "            return z\n",
    "\n",
    "        else:\n",
    "            log_det_W = BS*n_of_groups*W.logdet()\n",
    "            z = self.conv(z)\n",
    "\n",
    "            return z, log_det_W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WN(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the WaveNet like layer for the affine coupling.  The primary difference\n",
    "    from WaveNet is the convolutions need not be causal.  There is also no dilation\n",
    "    size reset.  The dilation only doubles on each layer\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in_channels, n_mel_channels, n_layers, n_channels,\n",
    "                 kernel_size):\n",
    "        super(WN, self).__init__()\n",
    "        assert kernel_size % 2 == 1\n",
    "        assert n_channels % 2 == 0\n",
    "        self.n_layers = n_layers\n",
    "        self.n_channels = n_channels\n",
    "        self.in_layers = nn.ModuleList()\n",
    "        self.res_skip_layers = nn.ModuleList()\n",
    "\n",
    "        start = nn.Conv1d(n_in_channels, n_channels, 1)\n",
    "        start = nn.utils.weight_norm(start, name='weight')\n",
    "        self.start = start\n",
    "\n",
    "        end = nn.Conv1d(n_channels, 2*n_in_channels, 1)\n",
    "        end.weight.data.zero_()\n",
    "        end.bias.data.zero_()\n",
    "        self.end = end\n",
    "\n",
    "        cond_layer = nn.Conv1d(n_mel_channels, 2*n_channels*n_layers, 1)\n",
    "        self.cond_layer = nn.utils.weight_norm(cond_layer, name='weight')\n",
    "\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            dilation = 2*i\n",
    "            padding = int(dilation * (kernel_size - 1)/2)\n",
    "            in_layer = nn.Conv1d(n_channels, 2*n_channels, kernel_size, padding=padding, dilation=dilation)\n",
    "            in_layer = nn.utils.weight_norm(in_layer, name='weight')\n",
    "            self.in_layers.append(in_layer)\n",
    "\n",
    "            if i < n_layers-1:\n",
    "                res_skip_channels = 2*n_channels\n",
    "            else:\n",
    "                res_skip_channels = n_channels\n",
    "            \n",
    "            res_skip_layer = nn.Conv1d(n_channels, res_skip_channels, 1)\n",
    "            res_skip_layer = nn.utils.weight_norm(res_skip_channels, name='weight')\n",
    "            self.res_skip_layers.append(res_skip_layer)\n",
    "\n",
    "        \n",
    "    def forward(self, forward_input):\n",
    "        audio, spec = forward_input\n",
    "        audio = self.start(audio)\n",
    "        output = torch.zeros_like(audio)\n",
    "        n_channels_tensor = torch.IntTensor([self.n_channels])\n",
    "\n",
    "        spec = self.cond_layer(spec)\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            spec_offset = i*2*self.n_channels\n",
    "            acts = fused_add_tanh_sigmoid(self.in_layers[i](audio),\n",
    "            spec[:,spec_offset:spec_offset+2*self.n_channels,:],\n",
    "            n_channels_tensor)\n",
    "            res_skip_acts = self.res_skip_layers[i](acts)\n",
    "            if i < self.n_layers - 1:\n",
    "                audio = audio + res_skip_acts[:,:self.n_channels,:]\n",
    "                output = output + res_skip_acts[:,self.n_channels:,:]\n",
    "            else:\n",
    "                output = output + res_skip_acts\n",
    "\n",
    "        return self.end(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(50, dtype=torch.int32)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WaveGlow(nn.Module):\n",
    "    def __init__(self, n_mel_channels, n_flows, n_group, n_early_every, n_early_size,\n",
    "                n_layers=8, n_channels=256, kernel_size=3):\n",
    "        super(WaveGlow, self).__init__()\n",
    "\n",
    "        self.upsample = nn.ConvTranspose1d(n_mel_channels, n_mel_channels, kernel_size=1024, stride=256)\n",
    "\n",
    "        assert n_group%2==0\n",
    "        self.n_flows = n_flows\n",
    "        self.n_group = n_group\n",
    "        self.n_early_every = n_early_every\n",
    "        self.n_early_size = n_early_size\n",
    "        self.WN = nn.ModuleList()\n",
    "        self.convinv = nn.ModuleList()\n",
    "\n",
    "        n_half = int(n_group/2)\n",
    "        for k in range(n_flows):\n",
    "            if k % self.n_early_every==0 and k > 0:\n",
    "                n_half = n_half - int(self.n_early_size/2)\n",
    "                n_remaining_channels = n_remaining_channels - self.n_early_size\n",
    "            self.convinv.append(Invertible1x1Conv(n_remaining_channels))\n",
    "            self.WN.append(WN(n_half, n_mel_channels*n_group, n_layers, n_channels, kernel_size))\n",
    "        self.n_remaining_channels = n_remaining_channels\n",
    "\n",
    "    def forward(self, forward_input):\n",
    "        \"\"\"\n",
    "        mel_spec: [BS, n_mel_channels, frames]\n",
    "        audio: [BS, time]\n",
    "        \"\"\"\n",
    "\n",
    "        spec, audio = forward_input\n",
    "        spec = self.upsample(spec)\n",
    "        \n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9414e94257f5df456230ea9212a9d2caf7ef208e33124ce6f17f0ac694c2c858"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
