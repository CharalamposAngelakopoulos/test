{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 125])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "x = torch.randn(1,1000)\n",
    "un = x.unfold(1,8,8).permute(0,2,1)\n",
    "un.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 416, 8])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsample = nn.ConvTranspose1d(80, 80, 1024, 256)\n",
    "spec = torch.randn(1,80,10)\n",
    "upsample(spec).unfold(2,8,8).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-236-c61bfe866b86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mspec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mspec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munfold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert(spec.size(2) >= x.size(1))\n",
    "if spec.size(2) >= x.size(1):\n",
    "    spec = spec[:,:,:x.size(1)]\n",
    "\n",
    "spec = spec.unfold(2,8,8).permute(0,2,1,3)\n",
    "spec = spec.contiguous().view(spec.size(0), spec.size(1), -1).permute(0,2,1)\n",
    "spec.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 125])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Invertible1x1Conv(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super(Invertible1x1Conv, self).__init__()\n",
    "        self.conv = nn.Conv1d(c, c, 1, bias=False)\n",
    "        w = self.conv.weight.data.squeeze()\n",
    "        w = nn.init.orthogonal_(torch.ones_like(w))\n",
    "\n",
    "        if w.det() < 0:\n",
    "            w[:,0] = -w[:,0]\n",
    "\n",
    "        self.conv.weight.data = w.unsqueeze(-1)\n",
    "        \n",
    "\n",
    "    def forward(self, z, reverse=False):\n",
    "        BS, group_size, n_of_groups = z.size()\n",
    "        W = self.conv.weight.squeeze()\n",
    "        if reverse:\n",
    "            W_inverse = W.float().inverse().unsqueeze(-1)\n",
    "            z = F.conv1d(z, W_inverse)\n",
    "            return z\n",
    "        else:\n",
    "            z = self.conv(z)\n",
    "            log_det_W = W.logdet()#BS*n_of_groups*\n",
    "            return z, log_det_W\n",
    "\n",
    "inv = Invertible1x1Conv(8)\n",
    "inv(torch.randn(1,8,125), True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):\n",
    "    n_channels_int = n_channels[0]\n",
    "    in_act = input_a + input_b\n",
    "    t_act = torch.tanh(in_act[:,:n_channels_int,:])\n",
    "    s_act = torch.sigmoid(in_act[:,n_channels_int:,:])\n",
    "    acts = t_act + s_act\n",
    "    return acts\n",
    "\n",
    "class WN(nn.Module):\n",
    "    def __init__(self, n_in_channels, n_mel_channels, n_layers,\n",
    "                 n_channels, kernel_size):\n",
    "        super(WN, self).__init__()\n",
    "        assert(kernel_size % 2 == 1)\n",
    "        assert(n_channels % 2 == 0)\n",
    "        self.n_layers = n_layers\n",
    "        self.n_channels = n_channels\n",
    "        self.in_layers = nn.ModuleList()\n",
    "        self.res_skip_layers = nn.ModuleList()\n",
    "\n",
    "        start = nn.Conv1d(n_in_channels, n_channels, 1)\n",
    "        start = nn.utils.weight_norm(start, name='weight')\n",
    "        self.start = start\n",
    "\n",
    "        end = nn.Conv1d(n_channels, 2*n_in_channels, 1)\n",
    "        end.weight.data.zero_\n",
    "        end.bias.data.zero_\n",
    "        self.end = end\n",
    "\n",
    "        cond_layer = nn.Conv1d(n_mel_channels, 2*n_mel_channels*n_layers, 1)\n",
    "        self.cond_layer = nn.utils.weight_norm(cond_layer, name='weight')\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            dilation = 2**i\n",
    "            padding = int(dilation*(kernel_size-1)/2)\n",
    "            in_layer = nn.Conv1d(n_channels, 2*n_channels, kernel_size,\n",
    "                                    dilation=dilation, padding=padding)\n",
    "            in_layer = nn.utils.weight_norm(in_layer, name='weight')\n",
    "            self.in_layers.append(in_layer)\n",
    "\n",
    "            if i < n_layers - 1:\n",
    "                res_skip_channels = 2*n_channels\n",
    "            else:\n",
    "                res_skip_channels = n_channels\n",
    "            \n",
    "            res_skip_layer = nn.Conv1d(n_channels, res_skip_channels, 1)\n",
    "            res_skip_layer = nn.utils.weight_norm(res_skip_layer, name='weight')\n",
    "            self.res_skip_layers.append(res_skip_layer)\n",
    "\n",
    "    def forward(self, forward_input):\n",
    "        audio, spect = forward_input\n",
    "        audio = self.start(audio)\n",
    "        output = torch.zeros_like(audio)\n",
    "        n_channels_tensor = torch.IntTensor([self.n_channels])\n",
    "\n",
    "        spect = self.cond_layer(spect)\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            spect_offset = i*2*self.n_channels\n",
    "            acts = fused_add_tanh_sigmoid_multiply(\n",
    "                self.in_layers[i](audio),\n",
    "                spect[:,spect_offset:spect_offset+2*self.n_channels,:],\n",
    "                n_channels_tensor)\n",
    "            \n",
    "            res_skip_acts = self.res_skip_layers[i](acts)\n",
    "            if i < self.n_layers - 1:\n",
    "                audio = audio + res_skip_acts[:,:self.n_channels,:]\n",
    "                output = output + res_skip_acts[:, self.n_channels:,:]\n",
    "            else:\n",
    "                output = output + res_skip_acts\n",
    "\n",
    "        return self.end(output)\n",
    "\n",
    "inv = Invertible1x1Conv(8)\n",
    "x = torch.randn(1,8,125)\n",
    "sp = torch.randn(1,80,125)\n",
    "wn = WN(4,80,8,256, 3)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveGlow(nn.Module):\n",
    "    def __init__(self, n_mel_channels, n_flows, n_group, n_early_every,\n",
    "                n_early_size, WN_config):\n",
    "        super(WaveGlow, self).__init__()\n",
    "\n",
    "        self.upsample = nn.ConvTranspose1d(n_mel_channels,\n",
    "                                           n_mel_channels,\n",
    "                                           1024, stride=256)\n",
    "        assert(n_group % 2 == 0)\n",
    "        self.n_flows = n_flows\n",
    "        self.n_group = n_group\n",
    "        self.n_early_every = n_early_every\n",
    "        self.n_early_size = n_early_size\n",
    "        self.WN = nn.ModuleList()\n",
    "        self.convinv = nn.ModuleList()\n",
    "\n",
    "        n_half = int(n_group/2)\n",
    "        n_remaining_channels = n_group\n",
    "        for k in range(n_flows):\n",
    "            if k % self.n_early_every == 0 and k > 0:\n",
    "                n_half = n_half - int(self.n_early_size/2)\n",
    "                n_remaining_channels = n_remaining_channels - self.n_early_size\n",
    "            self.convinv.append(Invertible1x1Conv(n_remaining_channels))\n",
    "            self.WN.append(WN(n_half, n_mel_channels*n_group, **WN_config))\n",
    "        self.n_remaining_channels = n_remaining_channels\n",
    "\n",
    "    def forward(self, forward_input):\n",
    "        \"\"\"\n",
    "        forward_input[0] = mel_spectrogram:  batch x n_mel_channels x frames\n",
    "        forward_input[1] = audio: batch x time\n",
    "        \"\"\"\n",
    "\n",
    "        spect, audio = forward_input\n",
    "        spect = self.upsample(spect)\n",
    "        assert(spect.size(2)>=audio.size(1))\n",
    "        if spect.size(2) > audio.size(1):\n",
    "            spect = spect[:,:,:audio.size(1)]\n",
    "        spect = spect.unfold(2, self.n_group, self.n_group).permute(0,2,1,3)\n",
    "        spect = spect.contiguous().view(spect.size(0), spect.size(1), -1).permute(0,2,1)\n",
    "        audio = audio.unfold(1, self.n_group, self.n_group).permute(0,2,1)\n",
    "\n",
    "        output_audio = []\n",
    "        log_s_list = []\n",
    "        log_det_W_list = []\n",
    "\n",
    "        print(audio.shape, spect.shape)\n",
    "        for k in range(self.n_flows):\n",
    "            if k % self.n_early_every == 0 and k > 0:\n",
    "                output_audio.append(audio[:, :self.n_early_size, :])\n",
    "                audio = audio[:,self.n_early_size:,:]\n",
    "            \n",
    "            audio, log_det_W = self.convinv[k](audio)\n",
    "            log_det_W_list.append(log_det_W)\n",
    "\n",
    "            n_half = int(audio.size(1)/2)\n",
    "            audio_0 = audio[:,:n_half,:]\n",
    "            audio_1 = audio[:,n_half:,:]\n",
    "\n",
    "            output = self.WN[k]((audio_0, spect))\n",
    "            log_s = output[:,n_half:,:]\n",
    "            b = output[:,:n_half,:]\n",
    "            audio_1 = torch.exp(log_s)*audio_1 + b\n",
    "            log_s_list.append(log_s)\n",
    "\n",
    "            audio = torch.cat([audio_0, audio_1], 1)\n",
    "\n",
    "        output_audio.append(audio)\n",
    "        return torch.cat(output_audio, 1), log_s_list, log_det_W_list\n",
    "        \n",
    "\n",
    "WN_config =  {\"n_layers\": 8,\n",
    "             \"n_channels\": 256,\n",
    "             \"kernel_size\": 3}\n",
    "            \n",
    "        \n",
    "waveglow = WaveGlow(80, 12, 8, 4, 2, WN_config)\n",
    "\n",
    "audio = torch.randn(1, 10000)\n",
    "spect = torch.randn(1,80, 100)\n",
    "waveglow.upsample(spect).shape\n",
    "#waveglow(( spect, audio))\n",
    "\n",
    "audio_0 = inv(audio.reshape(1,8,1250))[0][:,:4,:]\n",
    "audio_1 = inv(audio.reshape(1,8,1250))[0][:,4:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1280, 26368])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.cond_layer(waveglow.upsample(spect)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [512, 256, 3], expected input[1, 4, 1250] to have 256 channels, but got 4 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-271-09f0f12e3649>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio_0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1118\u001b[0m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1120\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    295\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m--> 297\u001b[1;33m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    298\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [512, 256, 3], expected input[1, 4, 1250] to have 256 channels, but got 4 channels instead"
     ]
    }
   ],
   "source": [
    "wn.in_layers[0](audio_0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9414e94257f5df456230ea9212a9d2caf7ef208e33124ce6f17f0ac694c2c858"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
